{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-24T15:14:28.693805Z",
     "end_time": "2023-04-24T15:14:28.711587Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from gym.spaces import Discrete\n",
    "from keras.losses import Huber\n",
    "from keras.optimizers.legacy.adam import Adam\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Experience:\n",
    "    def __init__(self, s, done, a, r, s_p):\n",
    "        self.state = s\n",
    "        self.done = done\n",
    "        self.action = a\n",
    "        self.reward = r\n",
    "        self.next_state = s_p\n",
    "\n",
    "        self._as_list = [s, a, r, s_p]\n",
    "        self._list_iter_counter = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._list_iter_counter += 1\n",
    "        if self._list_iter_counter < 4:\n",
    "            yield self._as_list[self._list_iter_counter - 1]\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.done\n",
    "\n",
    "\n",
    "class UniformReplayBuffer:\n",
    "    def __init__(self, max_length=10000, minibatch_size=32):\n",
    "        self.buf = []\n",
    "        self.max_len = max_length\n",
    "        self.mb_size = minibatch_size\n",
    "\n",
    "    def sample_minibatch(self):\n",
    "        if len(self.buf) <= self.mb_size:\n",
    "            raise ValueError(\"Not enough experiences in the buffer to sample a minibatch.\"\n",
    "                             \"Consider asserting that `buffer.num_experiences() > buffer.mb_size'\")\n",
    "\n",
    "        mb_start_idx = np.random.randint(0, len(self.buf) - self.mb_size)\n",
    "        return self.buf[mb_start_idx:mb_start_idx + self.mb_size]\n",
    "\n",
    "    def add_experience(self, experience: Experience):\n",
    "        if len(self.buf) >= self.max_len:\n",
    "            del_idx = np.random.randint(0, self.max_len)\n",
    "            self.buf[del_idx] = experience\n",
    "        else:\n",
    "            self.buf.append(experience)\n",
    "\n",
    "    def num_experiences(self):\n",
    "        return len(self.buf)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T15:14:31.467561Z",
     "end_time": "2023-04-24T15:14:31.486684Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class DqnAgent:\n",
    "    def __init__(self,\n",
    "                 obs_shape,  # size of the state |s|\n",
    "                 num_actions,  # number of actions in the environment\n",
    "                 alpha=1e-3,  # AdaM learning rate\n",
    "                 epsilon=0.1,  # random move probability\n",
    "                 gamma=0.99,  # discount factor\n",
    "                 qnet_fc_layer_params=(128, 64),  # neuron counts for the fully-connected layers of the Q Network\n",
    "                 qnet_conv_layer_params=(32, 64, 128),  # filter counts for convolutional layers of the Q network\n",
    "                 rng_seed: int = None,  # seed to RNG (optional, for debugging really)\n",
    "                 debug: bool = False  # enable debugging mode, useful for stack traces in tensorflow functions\n",
    "                 ):\n",
    "        # public fields\n",
    "\n",
    "        # prefix private (i.e. shouldn't be accessed from other classes) fields with underscore.\n",
    "        self._action_set = np.array(list(range(num_actions)))\n",
    "        self._alpha = np.float32(alpha)\n",
    "        self._epsilon = np.float32(epsilon)\n",
    "        self._gamma = np.float32(gamma)\n",
    "        self._rng = np.random.RandomState(seed=rng_seed)\n",
    "        self._debug = debug\n",
    "\n",
    "        self._qnet = tf.keras.models.Sequential()\n",
    "        self._qnet.add(tf.keras.layers.InputLayer(input_shape=obs_shape))\n",
    "\n",
    "        # if we need to use convolutional layers, add them and pooling layers after.\n",
    "        if qnet_conv_layer_params is not None:\n",
    "            for filter_count in qnet_conv_layer_params:\n",
    "                self._qnet.add(tf.keras.layers.Conv2D(filter_count, kernel_size=3, activation='relu'))\n",
    "                self._qnet.add(tf.keras.layers.MaxPool2D())\n",
    "                self._qnet.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "            self._qnet.add(tf.keras.layers.Flatten())\n",
    "\n",
    "        # add the fully-connected layers to the neural network.\n",
    "        for neuron_count in qnet_fc_layer_params:\n",
    "            self._qnet.add(tf.keras.layers.Dense(neuron_count, 'relu'))\n",
    "\n",
    "        # add the final layer, with linear activation\n",
    "        self._qnet.add(tf.keras.layers.Dense(num_actions, 'linear'))\n",
    "\n",
    "        self._qnet.compile(Adam(learning_rate=alpha), loss=Huber(), run_eagerly=self._debug)\n",
    "\n",
    "    def action(self, state):\n",
    "        # rolled less than epsilon. return random action.\n",
    "        if self._rng.random() < self._epsilon:\n",
    "            return self._rng.choice(self._action_set)\n",
    "\n",
    "        # choose a greedy action\n",
    "        # generate action q values by calling the network on the current state.\n",
    "        # qnet may expect a batched input, in which case we need to expand dims.\n",
    "        action_q_values = self._qnet(tf.expand_dims(state, axis=0))\n",
    "        action_q_values = tf.squeeze(action_q_values)\n",
    "\n",
    "        return tf.argmax(action_q_values).numpy()\n",
    "\n",
    "    def train(self, experience: Experience):\n",
    "        return self.train_on_batch([experience])\n",
    "        # s_prime = tf.expand_dims(experience.next_state, axis=0)\n",
    "        # a_prime = self._qnet(s_prime)\n",
    "        # max_q_for_a_prime = tf.squeeze(tf.reduce_max(a_prime)).numpy()\n",
    "        # ins = tf.expand_dims(experience.state, axis=0)\n",
    "        # info = tf.convert_to_tensor((np.float32(experience.action), np.float32(experience.reward),\n",
    "        #                             max_q_for_a_prime, np.float32(len(self._action_set)),\n",
    "        #                             self._gamma, self._alpha))\n",
    "        # info = tf.reshape(info, (1, -1))\n",
    "        #\n",
    "        # loss = self._qnet.train_on_batch(ins, y=info)\n",
    "        # return loss\n",
    "\n",
    "    def train_on_batch(self, batch: [Experience]):\n",
    "        # unpack the experience batch into arrays of state, reward, etc.\n",
    "        states = np.array(list(map(lambda x: x.state, batch)))\n",
    "        rewards = np.array(list(map(lambda x: x.reward, batch)))\n",
    "        s_primes = np.array(list(map(lambda x: x.next_state, batch)))\n",
    "\n",
    "        # convert s_primes to a tensor, run it through our q_network, and get the maximum action value for each s'\n",
    "        a_primes = self._qnet(tf.convert_to_tensor(s_primes))\n",
    "        max_q_prime = np.max(a_primes, axis=1)\n",
    "\n",
    "        # create a mask to apply to the max_q_prime array, because we don't want to consider the max_q value of the\n",
    "        #   next state if our state s is terminal\n",
    "        mask = np.array(list(map(lambda x: not x.is_terminal(), batch)), dtype=np.float32)\n",
    "        y_true = rewards + mask * self._gamma * max_q_prime\n",
    "\n",
    "        # finally, convert the above array to a tensor, and train our q_network on it\n",
    "        y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "        history = self._qnet.fit(x=states, y=y_true, shuffle=False, verbose=False)\n",
    "        return history.history['loss']\n",
    "\n",
    "    def set_epsilon(self, new_value) -> None:\n",
    "        # just good practice to have a setter rather than accessing it raw, because we are using conversions as\n",
    "        #   type checking is pretty important if we are going to try and run our code in TF graph mode.\n",
    "        self._epsilon = np.float32(new_value)\n",
    "\n",
    "    def save_policy(self):\n",
    "        nonce = np.random.randint(0, 1e6)\n",
    "        if not os.path.isdir('saved_configs'):\n",
    "            os.makedirs('saved_configs')\n",
    "        conf = open(f\"./saved_configs/dqn_conf_{nonce}.txt\", \"w+\")\n",
    "        params = vars(self)\n",
    "        for param in params:\n",
    "            conf.write(f\"{param}: {params[param]}\\n\")\n",
    "        self._qnet.save(f\"./saved_configs/dqn_qnet_{nonce}.h5\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T15:14:32.910279Z",
     "end_time": "2023-04-24T15:14:32.940800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def cartpole_test(num_episodes=150, render=True, verbose=False):\n",
    "\n",
    "    env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "    # reset the environment and get the initial state s0\n",
    "    state = env.reset()\n",
    "    if len(state) == 2 and type(state) == tuple:\n",
    "        state = state[0]\n",
    "\n",
    "    # create our DQN agent, passing it information about the environment's observation/action spec.\n",
    "    dqn_agent = DqnAgent(state.shape, env.action_space.n,\n",
    "                         qnet_conv_layer_params=None, epsilon=1e-3, alpha=1e-7, gamma=0.9)\n",
    "\n",
    "    replay_buffer = UniformReplayBuffer(max_length=10000, minibatch_size=128)\n",
    "\n",
    "    if render:\n",
    "        print(\"[WARN]: Rendering will slow down training. Are you sure you want to be rendering?\")\n",
    "\n",
    "    # while the episode isn't over, generate a new action on the state, perform that action, then train.\n",
    "    returns = np.zeros(num_episodes)\n",
    "    for ep in range(num_episodes):\n",
    "        ep_return = 0.\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # state is a tuple in CarRacing for some reason. just get the pixel-based observation.\n",
    "            if len(state) == 2 and type(state) == tuple:\n",
    "                state = state[0]\n",
    "                # call the action wrapper to get an e-greedy action\n",
    "            action = dqn_agent.action(state)\n",
    "\n",
    "            # run the action on the environment and get the new info\n",
    "            new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            # state is a tuple in CarRacing for some reason. just get the pixel-based observation.\n",
    "            if len(state) == 2 and type(state) == tuple:\n",
    "                new_state = new_state[0]\n",
    "            # add the experience to our replay buffer\n",
    "            experience = Experience(state, done, action, reward, new_state)\n",
    "            replay_buffer.add_experience(experience)\n",
    "\n",
    "            # render the environment\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # train on the experience\n",
    "            if not done:\n",
    "                if replay_buffer.mb_size < replay_buffer.num_experiences():\n",
    "                    training_batch = replay_buffer.sample_minibatch()\n",
    "                    loss = dqn_agent.train_on_batch(training_batch)\n",
    "\n",
    "                    if verbose:\n",
    "                        print(loss)\n",
    "\n",
    "            ep_return += reward\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        # episode terminated by this point\n",
    "        returns[ep] = ep_return\n",
    "        print(f\"Episode {ep} over. Total return: {ep_return}\")\n",
    "\n",
    "    plt.plot(returns)\n",
    "    _ = plt.title(\"Agent total returns per episode (Training)\"), plt.xlabel(\"Episode\"), plt.ylabel(\"Return\")\n",
    "    plt.show()\n",
    "    return dqn_agent, returns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T15:14:33.911128Z",
     "end_time": "2023-04-24T15:14:33.943749Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def run_dqn_on_env(env: gym.Env, num_episodes=150, render=True, verbose=False):\n",
    "    assert type(env.action_space) == Discrete, \"Can only use this DQN Agent on discrete action spaces for now.\"\n",
    "\n",
    "    # reset the environment and get the initial state s0\n",
    "    state = env.reset()\n",
    "    if len(state) == 2 and type(state) == tuple:\n",
    "        state = state[0]\n",
    "    # create our DQN agent, passing it information about the environment's observation/action spec.\n",
    "    dqn_agent = DqnAgent(state.shape, env.action_space.n, qnet_fc_layer_params=(256, 256, 128), epsilon=0.2, gamma=0.95)\n",
    "\n",
    "    replay_buffer = UniformReplayBuffer(max_length=10000, minibatch_size=32)\n",
    "\n",
    "    if render:\n",
    "        print(\"[WARN]: Rendering will slow down training. Are you sure you want to be rendering?\")\n",
    "\n",
    "    # while the episode isn't over, generate a new action on the state, perform that action, then train.\n",
    "    returns = np.zeros(num_episodes)\n",
    "    for ep in range(num_episodes):\n",
    "        ep_return = 0.\n",
    "        state = env.reset()\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            # state is a tuple in CarRacing for some reason. just get the pixel-based observation.\n",
    "            if len(state) == 2 and type(state) == tuple:\n",
    "                state = state[0]\n",
    "                # call the action wrapper to get an e-greedy action\n",
    "            action = dqn_agent.action(state)\n",
    "\n",
    "            # run the action on the environment and get the new info\n",
    "            new_state, reward, truncated, done, info = env.step(action)\n",
    "\n",
    "            # state is a tuple in CarRacing for some reason. just get the pixel-based observation.\n",
    "            if len(state) == 2 and type(state) == tuple:\n",
    "                new_state = new_state[0]\n",
    "            # add the experience to our replay buffer\n",
    "            experience = Experience(state, done, action, reward, new_state)\n",
    "            replay_buffer.add_experience(experience)\n",
    "\n",
    "            # render the environment\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # train on the experience\n",
    "            if not done:\n",
    "                if replay_buffer.mb_size < replay_buffer.num_experiences():\n",
    "                    training_batch = replay_buffer.sample_minibatch()\n",
    "                    loss = dqn_agent.train_on_batch(training_batch)\n",
    "\n",
    "                    if verbose:\n",
    "                        print(loss)\n",
    "\n",
    "            ep_return += reward\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        # episode terminated by this point\n",
    "        returns[ep] = ep_return\n",
    "        print(f\"Episode {ep} over. Total return: {ep_return}\")\n",
    "\n",
    "    plt.plot(returns)\n",
    "    _ = plt.title(\"Agent total returns per episode (Training)\"), plt.xlabel(\"Episode\"), plt.ylabel(\"Return\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # print(returns)\n",
    "    return dqn_agent, returns\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T15:14:34.864319Z",
     "end_time": "2023-04-24T15:14:34.903269Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_agent_on_env(agent: DqnAgent, env: gym.Env, num_eval_episodes=100, render=True):\n",
    "    # set the agent's random move probability to 0, so we can evaluate its policy exclusively.\n",
    "    # agent MUST expose a set_epsilon method to control randomness, hence we type-hint that it is a DqnAgent\n",
    "    agent.set_epsilon(0.0)\n",
    "\n",
    "    eval_returns = np.zeros(num_eval_episodes)\n",
    "    for eval_ep in range(num_eval_episodes):\n",
    "        # reset our environment for this run\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_return = 0.\n",
    "\n",
    "        while not done:\n",
    "            # call the action wrapper to get an e-greedy action\n",
    "            action = agent.action(state)\n",
    "\n",
    "            # run the action on the environment and get the new info\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # render the environment\n",
    "            if render:\n",
    "                env.render('human')\n",
    "\n",
    "            ep_return += reward\n",
    "            state = new_state\n",
    "\n",
    "        # episode terminated by this point\n",
    "        eval_returns[eval_ep] = ep_return\n",
    "        print(f\"Evaluation episode {eval_ep} over. Total return: {ep_return}\")\n",
    "\n",
    "    return eval_returns\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T15:14:35.647229Z",
     "end_time": "2023-04-24T15:14:35.677043Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # a really simple example observation and action space. Importantly, num_observations is the size of a state\n",
    "    #    and num_actions is the number of actions available in the environment.\n",
    "\n",
    "    example_state = (0., 0., 1.)\n",
    "    example_action_set = list(range(5))\n",
    "\n",
    "    # we need to instruct the DQN Agent to not use convolutional layers, otherwise it will expect pixel-shaped inputs.\n",
    "    agent = DqnAgent(obs_shape=(3, ), num_actions=len(example_action_set), qnet_conv_layer_params=None,\n",
    "                     epsilon=1., rng_seed=150)\n",
    "    print(f\"Sampling a random action: {agent.action(example_state)}\")\n",
    "\n",
    "    agent.set_epsilon(0.)\n",
    "    print(f\"Sampling a greedy action: {agent.action(example_state)}\")\n",
    "\n",
    "    # train an agent on a given environment\n",
    "    test_env = gym.envs.make('CarRacing-v2',\n",
    "                             continuous=False, render_mode='human'\n",
    "                             )\n",
    "\n",
    "    trained_agent, returns = run_dqn_on_env(test_env, num_episodes=1000, render=True)\n",
    "\n",
    "    # evaluate the agent on the same environment\n",
    "    eval_returns = evaluate_agent_on_env(trained_agent, test_env, num_eval_episodes=250, render=False)\n",
    "    plt.plot(eval_returns)\n",
    "    plt.title(\"Agent total returns per episode (Evaluation)\"), plt.xlabel(\"Eval. episode\"), plt.ylabel(\"Returns\")\n",
    "    plt.show()\n",
    "\n",
    "    # trained_agent.save_policy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T15:14:37.037876Z",
     "end_time": "2023-04-24T15:14:37.058927Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling a random action: 3\n",
      "Sampling a greedy action: 2\n",
      "[WARN]: Rendering will slow down training. Are you sure you want to be rendering?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf-gpu/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "2023-04-24 15:14:41.104517: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-04-24 15:14:41.456982: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
